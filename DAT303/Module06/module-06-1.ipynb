{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAT303 - Spring 2024 - Module 6.1 Notebook\n",
    "---\n",
    "Name:    \n",
    "Date:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> It is assumed you are using the module6 conda environment specified in the *module6.yaml* file downloaded from Canvas. Be sure to read all cells in this notebook. You are only to provide code in cells that contain `##### YOUR CODE HERE #####` and written responses in cells that contain `YOUR WRITTEN RESPONSE HERE`. Ensure that code cells are executed sequentially to prevent unexpected errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Metrics \n",
    "\n",
    "Machine learning classification metrics are used to evaluate the performance of classification models. These metrics provide insights into how well a model is able to classify instances into different classes. Recall from earlier the definitions of True Positive, False Positive, True Negative and False Negative:\n",
    "\n",
    "- True Positive (TP): TP represents the number of instances that were correctly predicted as positive by the classifier.\n",
    "In binary classification, a true positive occurs when the model predicts a sample as positive and the actual label is positive.\n",
    "\n",
    "- True Negative (TP): TN represents the number of instances that were correctly predicted as negative by the classifier.\n",
    "In binary classification, a true negative occurs when the model predicts a sample as negative and the actual label is negative.\n",
    "\n",
    "- False Positive (FP): FP represents the number of instances that were incorrectly predicted as positive by the model when they are actually negative. In binary classification, a false positive occurs when the model predicts a sample as positive but the actual label is negative.\n",
    "\n",
    "- False Negative (FN): FN represents the number of instances that were incorrectly predicted as negative by the model when they are actually positive. In binary classification, a false negative occurs when the model predicts a sample as negative but the actual label is positive.\n",
    "\n",
    "\n",
    "Note that the metrics that follow are applicable to binary classification tasks. There are multi-class extensions, but they will not be covered here.\n",
    "\n",
    "\n",
    "Accuracy:\n",
    "\n",
    "Accuracy is a common evaluation metric used to measure the performance of a classification model. It represents the proportion of correctly classified instances out of the total number of instances evaluated.\n",
    "Mathematically, accuracy is calculated as the ratio of the number of correctly predicted instances (true positives and true negatives) to the total number of instances:\n",
    "\n",
    "$$\n",
    "\\mathrm{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "$$\n",
    "\n",
    "- Pros: Simple and widely understood, provides an overall measure of correct predictions.\n",
    "- Cons: May not be the most appropriate metric for evaluating model performance in certain situations, particularly when dealing with imbalanced datasets or when the costs of false positives and false negatives are unequal.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Precision (Positive Predictive Value)\n",
    "\n",
    "recall measures the proportion of correctly predicted positive instances among all instances predicted as positive. It focuses on minimizing false positives:\n",
    "\n",
    "$$\n",
    "\\mathrm{Precision} = \\frac{TP}{TP + FP}\n",
    "$$\n",
    "\n",
    "\n",
    "- Pros: Indicates the proportion of correctly predicted positive instances among all instances predicted as positive, useful when minimizing false positives is critical.\n",
    "- Cons: Ignores true negatives.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "Recall (Sensitivity, True Positive Rate):\n",
    "\n",
    "Recall measures the ability of the model to correctly identify positive instances out of all actual positive instances. It focuses on minimizing false negatives:\n",
    "\n",
    "$$\n",
    "\\mathrm{Recall} = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "\n",
    "- Pros: Measures the ability of the model to correctly identify positive instances out of all actual positive instances, useful when minimizing false negatives is crucial.\n",
    "\n",
    "- Cons: Does not account for true negatives.\n",
    "\n",
    "<br>\n",
    "\n",
    "F1-Score:\n",
    "\n",
    "F1-Score is the harmonic mean of precision and recall. It provides a balance between precision and recall:\n",
    "\n",
    "$$\n",
    "\\mathrm{F1} = \\frac{2 \\times \\mathrm{Precision} \\times \\mathrm{Recall}}{\\mathrm{Precision} + \\mathrm{Recall}}\n",
    "$$\n",
    "​\n",
    "- Pros: Provides a balance between the two metrics.\n",
    "- Cons: Ignores true negatives.\n",
    "\n",
    "<br>\n",
    "\n",
    "Area Under ROC Curve (AUC-ROC):\n",
    "\n",
    "Receiver Operating Characteristic (ROC) curve plots the true positive rate (TPR) against the false positive rate (FPR) at various classification thresholds. The area under the ROC Curve (AUC) quantifies the model's ability to distinguish between classes across different classifier thresholds.\n",
    "\n",
    "\n",
    "![roc](../Module06/roc_curve.png)\n",
    "\n",
    "- Pros: Represents the model's ability to distinguish between positive and negative classes across various thresholds.\n",
    "- Cons: Requires binary classification and may not be well-suited for multi-class problems.\n",
    "\n",
    "\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "The confusion matrix is a fundamental tool in the evaluation of classification models, providing a visual representation of the performance of an algorithm. It's essentially a table used to describe the performance of a classification model on a set of test data for which the true values are known:\n",
    "\n",
    "![cm](../Module06/confusion_matrix.png)\n",
    "\n",
    "The confusion matrix provides a detailed breakdown of the model's performance, including true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN), offering insight into not just the errors made by the model but the  types of errors. Many other performance metrics, such as precision, recall, F1 score, and accuracy, can be derived from the confusion matrix, making it a versatile tool for evaluating classification models.\n",
    "\n",
    "\n",
    "### Choosing the Best Metric for a Problem \n",
    "\n",
    "Choosing between the best metric depends on the specific context and objectives of your machine learning project, as well as the costs associated with false positives (FP) and false negatives (FN). \n",
    "\n",
    "If the cost/impact of a false positive is high, prioritize precision. This is often the case in scenarios where the action taken on a positive prediction has significant consequences. For example, in email spam detection a high precision is preferred to avoid marking important emails as spam.\n",
    "Precision should also be preferred when the quality of the positive predictions is more important than capturing all positive instances. For instance, in a marketing campaign, you might prefer to only target users who are very likely to convert, even if it means missing out on some potential converters.\n",
    "\n",
    "\n",
    "If the cost/impact of a false negative is higher, recall becomes more important. This is typical in scenarios where failing to identify a positive instance has severe implications. For instance, in medical diagnoses for serious conditions, a high recall is critical to ensure that as many true cases as possible are identified, even at the risk of false positives.\n",
    "\n",
    "Another situation in which recall would be preferred is when it’s important to identify all possible positive instances, even at the expense of accuracy. For example, in fraud detection, it might be preferable to flag as many potential fraudulent transactions as possible for further investigation, even if doing so results in flagging many non-fraudulent transactions.\n",
    "\n",
    "If neither false positives nor false positives are costlier, f1-score is a good choice. Accuracy is seldom the best option unless you're dealing with perfectly balanced data.\n",
    "\n",
    "\n",
    "\n",
    "### Changing Classifier Threshold\n",
    "\n",
    "It is important to realize that as the discrimination threshold for a classifier is changed (which is .50 by default in scikit-learn), precision and recall also change. Keep in mind the following relationships:\n",
    "\n",
    "As the discrimination threshold increases:\n",
    "\n",
    "- The number of True Positives decreases or stays the same.\n",
    "- The number of True Negatives increases or stays the same. \n",
    "- The number of False Positives decreases or stays the same.\n",
    "- The number of False Negatives increases or stays the same.\n",
    "- Precision increases or stays the same (since False Positives generally decrease)\n",
    "- Recall decreases or stays the same (since True Positives typically decrease and False Negatives increase)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "Complete the following questions. Use the synthetically generated `yact` (target/ground truth) and `yhat` (modeled output/ predicted class) below for each metric evaluation. Do not use for loops in your metric functions: Take advantage of Numpy's broadcasting. \n",
    "\n",
    "More information on scikit-learn metrics can be found [here](https://scikit-learn.org/stable/modules/classes.html#classification-metrics).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the next cell to load `yact` and `yhat`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Generate samples for metric evaluation.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(516)\n",
    "yact = np.sort(rng.choice([0, 1], size=20, p=[.7, .3]))\n",
    "yhat = rng.choice([0, 1], size=20, p=[.75, .25])\n",
    "\n",
    "print(f\"yact : {yact}\")\n",
    "print(f\"yhat : {yhat}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "1. Implement a function named `my_accuracy`. It should accept `yact` and `yhat` as arguments and return a single scalar value representing the classifier's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "2. Implement a function named `my_precision`. It should accept `yact` and `yhat` as arguments and return a single scalar value representing the classifier's precision.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "3. Implement a function named `my_recall`. It should accept `yact` and `yhat` as arguments and return a single scalar value representing the classifier's recall.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "4. Implement a function named `my_f1`. It should accept `yact` and `yhat`, and return a single scalar value representing the classifier's f1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "5. Implement a function named `sklearn_accuracy`. It should accept `yact` and `yhat`, and return the value from the `accuracy_score` function in scikit-learn (should only require 1-2 lines of code including the return statement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Implement a function named `sklearn_precision`. It should accept `yact` and `yhat`, and return the value from the `precision_score` function in scikit-learn (should only require 1-2 lines of code including the return statement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "7. Implement a function named `sklearn_recall`. It should accept `yact` and `yhat`, and return the value from the `recall_score` function in scikit-learn (should only require 1-2 lines of code including the return statement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "8. Implement a function named `sklearn_f1`. It should accept `yact` and `yhat`, and return the value from the `f1_score` function in scikit-learn (should only require 1-2 lines of code including the return statement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##### YOUR CODE HERE #####\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Run the next cell to ensure outputs match for each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tol = 1e-9\n",
    "\n",
    "acc1 = my_accuracy(yact, yhat)\n",
    "acc2 = sklearn_accuracy(yact, yhat)\n",
    "\n",
    "pr1 = my_precision(yact, yhat)\n",
    "pr2 = sklearn_precision(yact, yhat)\n",
    "\n",
    "re1 = my_recall(yact, yhat)\n",
    "re2 = sklearn_recall(yact, yhat)\n",
    "\n",
    "f11 = my_f1(yact, yhat)\n",
    "f12 = sklearn_f1(yact, yhat)\n",
    "\n",
    "\n",
    "if abs(acc1 - acc2) <= tol:\n",
    "    print(f\"+ Outputs match for my_accuracy vs. sklearn_accuracy ({acc1:.5f} vs. {acc2:.5f}) [OK]\")\n",
    "else:\n",
    "    print(f\"- Outputs do not match for my_accuracy vs. sklearn_accuracy ({acc1:.5f} vs. {acc2:.5f}) [Not OK]\")\n",
    "\n",
    "if abs(pr1 - pr2) <= tol:\n",
    "    print(f\"+ Outputs match for my_precision vs. sklearn_precision ({pr1:.5f} vs. {pr2:.5f}) [OK]\")\n",
    "else:\n",
    "    print(f\"- Outputs do not match for my_precision vs. sklearn_precision ({pr1:.5f} vs. {pr2:.5f}) [Not OK]\")\n",
    "\n",
    "if abs(re1 - re2) <= tol:\n",
    "    print(f\"+ Outputs match for my_recall vs. sklearn_recall ({re1:.5f} vs. {re2:.5f})  [OK]\")\n",
    "else:\n",
    "    print(f\"- Outputs do not match for my_recall vs. sklearn_recall ({re1:.5f} vs. {re2:.5f})  [Not OK]\")\n",
    "\n",
    "if abs(f11 - f12) <= tol:\n",
    "    print(f\"+ Outputs match for my_f1 vs. sklearn_f1 ({f11:.5f} vs. {f12:.5f})  [OK]\")\n",
    "else:\n",
    "    print(f\"- Outputs do not match for my_f1 vs. sklearn_f1 ({f11:.5f} vs. {f12:.5f})  [Not OK]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Pick one classification metric not covered in this exercise (full list [here](https://scikit-learn.org/stable/modules/classes.html#classification-metrics)), and briefly describe a situation or modeling context in which this new metric might be preferrable to accuracy, precision, recall or F1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR WRITTEN RESPONSE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "module4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
